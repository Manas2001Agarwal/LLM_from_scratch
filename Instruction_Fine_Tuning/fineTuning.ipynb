{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git version"
      ],
      "metadata": {
        "id": "CsB9vR7xpTqU",
        "outputId": "2ab16d3f-d157-4e83-945a-39c274a35ee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "git version 2.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Manas Agarwal\"\n",
        "!git config --global user.email \"manasmrt10@gmail.com\""
      ],
      "metadata": {
        "id": "Sr7fNGDnpvKB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Manas2001Agarwal/LLM_from_scratch.git"
      ],
      "metadata": {
        "id": "6gDYNSlHp19s",
        "outputId": "6b701a5a-524e-4127-9df8-711e3ccecbaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM_from_scratch'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 88 (delta 31), reused 55 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (88/88), 634.98 KiB | 2.41 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLM_from_scratch/Instruction_Fine_Tuning"
      ],
      "metadata": {
        "id": "i0OL5hdKqLIj",
        "outputId": "0b87f7f4-7106-40c8-d5f0-dbf4378dcd96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLM_from_scratch/Instruction_Fine_Tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "QNOg34xIqiIc",
        "outputId": "4ca1e32e-da56-4af9-9d9c-10acd12a5307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_W-McCDoucX"
      },
      "source": [
        "##### Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fzERg6hWoucZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CZOssNeQouca"
      },
      "outputs": [],
      "source": [
        "def download_and_load_file(file_path, url):\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B4hJwNkdoucb",
        "outputId": "837e816c-83cd-48bc-9740-eba541f9d65d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "URpRElZtoucb",
        "outputId": "63c4981a-6959-435d-8c51-7611a8e50771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Identify the correct spelling of the following word.',\n",
              " 'input': 'Ocassion',\n",
              " 'output': \"The correct spelling is 'Occasion.'\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data[50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KpsPZE_doucb"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else ''\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "97n2hnT0oucc",
        "outputId": "14d26849-0316-476b-b3db-bcd98a99bb29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "### Input:\n",
            "Ocassion\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ],
      "source": [
        "def return_response(entry):\n",
        "    response = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "    return response\n",
        "\n",
        "print(format_input(data[50]) + return_response(data[50]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDRZRaqoucc"
      },
      "source": [
        "Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NO_9a_I6oucc",
        "outputId": "1598f63c-0540-469e-b444-c25a461728fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "935\n",
            "110\n",
            "55\n"
          ]
        }
      ],
      "source": [
        "train_portion = int(len(data) * 0.85)\n",
        "test_portion = int(len(data) * 0.10)\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion+test_portion]\n",
        "validation_data = data[train_portion+test_portion:]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "print(len(validation_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "L0skicEroucc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self,data,tokenizer):\n",
        "        self.data = data\n",
        "        self.encoded_texts = []\n",
        "        for entry in self.data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response\n",
        "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ChTXtjj6oucd"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch,pad_token = 50256,device = \"cpu\",ignore_index = -100,allowed_max_length = None):\n",
        "    input_lst, target_lst = [],[]\n",
        "    batch_max_length = max([len(input_tokens)+1 for input_tokens in batch])\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        new_item += [pad_token]\n",
        "\n",
        "        padded = new_item + [pad_token]*(batch_max_length-len(new_item))\n",
        "        input_padded = torch.tensor(padded[:-1])\n",
        "        target_padded = torch.tensor(padded[1:])\n",
        "\n",
        "        mask = (target_padded==pad_token)\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            target_padded[indices[1:]] = ignore_index\n",
        "\n",
        "        if allowed_max_length is not None:\n",
        "            target_padded = target_padded[:allowed_max_length]\n",
        "            input_padded = input_padded[:allowed_max_length]\n",
        "\n",
        "        input_lst.append(input_padded)\n",
        "        target_lst.append(target_padded)\n",
        "\n",
        "    input_lst = torch.stack(input_lst).to(device)\n",
        "    target_lst = torch.stack(target_lst).to(device)\n",
        "\n",
        "    return input_lst,target_lst\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JUIJqSJIoucd",
        "outputId": "72f95221-4885-432f-dca0-6fd8a54fd931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    0,     1,     2,     3,     4],\n",
              "         [    5,     6, 50256, 50256, 50256],\n",
              "         [    7,     8,     9, 50256, 50256]]),\n",
              " tensor([[    1,     2,     3,     4, 50256],\n",
              "         [    6, 50256,  -100,  -100,  -100],\n",
              "         [    8,     9, 50256,  -100,  -100]]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "custom_collate_fn(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKqq5MNHoucd"
      },
      "outputs": [],
      "source": [
        "# if torch.backends.mps.is_available():\n",
        "#     device = torch.device(\"mps\")\n",
        "\n",
        "# print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "abY49Mquoucd"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pHIQm-QGoucd"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "customized_collate_func = partial(\n",
        "    custom_collate_fn,\n",
        "    device = device,\n",
        "    allowed_max_length = 1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cuTItlj9oucd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data,tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    collate_fn=customized_collate_func,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "val_dataset = InstructionDataset(validation_data,tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    collate_fn=customized_collate_func,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data,tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    collate_fn=customized_collate_func,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F20brml3ouce",
        "outputId": "cfc54d90-b904-4d6f-882e-4753fb45a08e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loader\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([6, 76]) torch.Size([6, 76])\n"
          ]
        }
      ],
      "source": [
        "print(\"Validation Loader\")\n",
        "for input_token,target_tokens in val_loader:\n",
        "    print(input_token.size(),target_tokens.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ogh84Couce"
      },
      "source": [
        "Loading the Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CjC6THEMouce",
        "outputId": "ee9a2938-e29f-4b65-bb13-e9b63cfa925c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "96c714c6fbea4defa7624d27f92a1b81",
            "ff0eed8893db43caa1187899a3748cba",
            "f749676abf2f4a64bf7a86fde29cd927",
            "b8d9b0338a954990a404d633a1970b7d",
            "15f380cce4504d2f9436d37c3ca10e6a",
            "0dc94455d2ec44e691f4ea160af745bb",
            "51c6278f294e4714af5b3ab70a29e924",
            "92aeeb9100314a33abd1566e225ee5d7",
            "0c0aa9e6437442cdb580f76b06c20f53",
            "27100473cb524661beb0b3028fdc72f9",
            "2d822c0b58f5460bb8a10879d54ace6f",
            "0b980325f813454d9c0612de0e1c58e4",
            "a3cf5921602e41f399895c404d3c562d",
            "606da1500bda488fb01e37d20fb8616a",
            "d0f5319126284db39e0a7d3089b10b51",
            "72c265b1be13411396ca8921a23bd53f",
            "58c5c9e55a2c4773a213aa2f2a22af36",
            "e6cd6068f8be42c2b1e9cd4f680edeb4",
            "f35efe4d1412486fa9cef2f8321e8789",
            "19e78a4620024d9ca8e0ca5287b9f35a",
            "9dfa9222ef394b129c0364437b1e7fc1",
            "42cdc8cb3eab4a5ebf181ce9fac7cace"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96c714c6fbea4defa7624d27f92a1b81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b980325f813454d9c0612de0e1c58e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (wte): Embedding(50257, 1024)\n",
              "  (wpe): Embedding(1024, 1024)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (h): ModuleList(\n",
              "    (0-23): 24 x GPT2Block(\n",
              "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): GPT2SdpaAttention(\n",
              "        (c_attn): Conv1D(nf=3072, nx=1024)\n",
              "        (c_proj): Conv1D(nf=1024, nx=1024)\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): GPT2MLP(\n",
              "        (c_fc): Conv1D(nf=4096, nx=1024)\n",
              "        (c_proj): Conv1D(nf=1024, nx=4096)\n",
              "        (act): NewGELUActivation()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from transformers import GPT2Model\n",
        "\n",
        "model_names = {\n",
        "    \"gpt2-small (124M)\": \"openai-community/gpt2\",\n",
        "    \"gpt2-medium (355M)\": \"openai-community/gpt2-medium\",\n",
        "    \"gpt2-large (774M)\": \"openai-community/gpt2-large\",\n",
        "    \"gpt2-xl (1558M)\": \"openai-community/gpt2-xl\"\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "gpt_hf = GPT2Model.from_pretrained(model_names[CHOOSE_MODEL],cache_dir='checkpoints')\n",
        "gpt_hf.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UwHyYhJcouce"
      },
      "outputs": [],
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"drop_rate\": 0.0,       # Dropout rate\n",
        "    \"qkv_bias\": True        # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jjho4RO6ouce"
      },
      "outputs": [],
      "source": [
        "def assign_check(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(right.clone().detach())\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_weights(gpt, gpt_hf):\n",
        "\n",
        "    d = gpt_hf.state_dict()\n",
        "\n",
        "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
        "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
        "\n",
        "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
        "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign_check(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign_check(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign_check(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign_check(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign_check(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign_check(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign_check(gpt.trf_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign_check(gpt.trf_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign_check(gpt.trf_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign_check(gpt.trf_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign_check(gpt.trf_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign_check(gpt.trf_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign_check(gpt.trf_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign_check(gpt.trf_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign_check(gpt.trf_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign_check(gpt.trf_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n",
        "\n",
        "        gpt.final_norm.scale = assign_check(gpt.final_norm.scale, d[f\"ln_f.weight\"])\n",
        "        gpt.final_norm.shift = assign_check(gpt.final_norm.shift, d[f\"ln_f.bias\"])\n",
        "        gpt.out_head.weight = assign_check(gpt.out_head.weight, d[\"wte.weight\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jMvrqsMcoucf",
        "outputId": "ef330615-b8db-4dd4-c65f-31f75a9a8d0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLM_from_scratch\n"
          ]
        }
      ],
      "source": [
        "%pwd\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "K7_tAKKzoucf",
        "outputId": "37b8322e-eb26-4a4e-dd49-ac7564ac2a77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ],
      "source": [
        "from making_LLM_from_scratch.Scripts.gpt_archietecture import GPTModel\n",
        "gpt = GPTModel(BASE_CONFIG)\n",
        "\n",
        "load_weights(gpt, gpt_hf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pzcJTSwzoucf",
        "outputId": "99244543-9807-40e6-dd42-ed993711d6d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.825909376144409\n",
            "3.915006160736084\n"
          ]
        }
      ],
      "source": [
        "from making_LLM_from_scratch.Scripts.training_gpt import training_gpt, calc_loss_loader\n",
        "gpt.to(device)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader,gpt,device,num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader,gpt,device,num_batches=5)\n",
        "\n",
        "print(train_loss)\n",
        "print(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9GvEUPiGoucf",
        "outputId": "8b1c09c9-d240-4ae5-8b4a-90d8e5d4bda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.624, Val loss 2.649\n",
            "Ep 1 (Step 000005): Train loss 1.072, Val loss 1.088\n",
            "Ep 1 (Step 000010): Train loss 0.908, Val loss 0.929\n",
            "Ep 1 (Step 000015): Train loss 0.845, Val loss 0.870\n",
            "Ep 1 (Step 000020): Train loss 0.797, Val loss 0.832\n",
            "Ep 1 (Step 000025): Train loss 0.767, Val loss 0.815\n",
            "Ep 1 (Step 000030): Train loss 0.744, Val loss 0.803\n",
            "Ep 1 (Step 000035): Train loss 0.709, Val loss 0.778\n",
            "Ep 1 (Step 000040): Train loss 0.692, Val loss 0.778\n",
            "Ep 1 (Step 000045): Train loss 0.673, Val loss 0.770\n",
            "Ep 1 (Step 000050): Train loss 0.657, Val loss 0.760\n",
            "Ep 1 (Step 000055): Train loss 0.639, Val loss 0.744\n",
            "Ep 1 (Step 000060): Train loss 0.625, Val loss 0.732\n",
            "Ep 1 (Step 000065): Train loss 0.610, Val loss 0.731\n",
            "Ep 1 (Step 000070): Train loss 0.592, Val loss 0.729\n",
            "Ep 1 (Step 000075): Train loss 0.576, Val loss 0.728\n",
            "Ep 1 (Step 000080): Train loss 0.565, Val loss 0.725\n",
            "Ep 1 (Step 000085): Train loss 0.551, Val loss 0.714\n",
            "Ep 1 (Step 000090): Train loss 0.542, Val loss 0.709\n",
            "Ep 1 (Step 000095): Train loss 0.532, Val loss 0.703\n",
            "Ep 1 (Step 000100): Train loss 0.522, Val loss 0.699\n",
            "Ep 1 (Step 000105): Train loss 0.507, Val loss 0.691\n",
            "Ep 1 (Step 000110): Train loss 0.493, Val loss 0.684\n",
            "Ep 1 (Step 000115): Train loss 0.479, Val loss 0.683\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-018aed2f6087>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m train_losses, val_losses, tokens_seen = training_gpt( gpt, train_loader, val_loader, optimizer, \n\u001b[0m\u001b[1;32m     15\u001b[0m                                                      \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                      \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/LLM_from_scratch/making_LLM_from_scratch/Scripts/training_gpt.py\u001b[0m in \u001b[0;36mtraining_gpt\u001b[0;34m(model, train_dl, val_dl, optimizer, device, eval_freq, eval_iter, num_epochs, start_context, tokenizer)\u001b[0m\n\u001b[1;32m    115\u001b[0m                  f\"Val loss {val_loss:.3f}\")\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mgenerate_and_print_simple_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrack_train_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrack_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrack_token_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/LLM_from_scratch/making_LLM_from_scratch/Scripts/training_gpt.py\u001b[0m in \u001b[0;36mgenerate_and_print_simple_text\u001b[0;34m(start_context, tokenizer, model)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mcontent_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     output_id = generate_simple_text(model,idx,max_tokens=50,\n\u001b[0m\u001b[1;32m     85\u001b[0m                                      content_size = content_size)\n\u001b[1;32m     86\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_ids_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/LLM_from_scratch/making_LLM_from_scratch/Scripts/training_gpt.py\u001b[0m in \u001b[0;36mgenerate_simple_text\u001b[0;34m(model, idx, max_tokens, content_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcontent_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtoken_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/LLM_from_scratch/making_LLM_from_scratch/Scripts/gpt_archietecture.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mtok_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mpos_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_embeds\u001b[0m  \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ],
      "source": [
        "PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
        "import time\n",
        "\n",
        "import torch.optim.optimizer\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    gpt.parameters(),lr = 0.00005, weight_decay=0.1\n",
        ")\n",
        "num_epochs = 2\n",
        "train_losses, val_losses, tokens_seen = training_gpt( gpt, train_loader, val_loader, optimizer,\n",
        "                                                     device, num_epochs=num_epochs, eval_freq=5,\n",
        "                                                     eval_iter=5,\n",
        "                                                     start_context=format_input(validation_data[0]),\n",
        "                                                     tokenizer=tokenizer)\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (start_time - end_time)/60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_97cmrLoucf"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from making_LLM_from_scratch.Scripts.training_gpt import text_to_token_ids,token_ids_to_text,generate_simple_text\n",
        "for i,entry in tqdm(enumerate(test_data),total = len(test_data)):\n",
        "    instruction = format_input(entry)\n",
        "\n",
        "    response_token_ids = generate_simple_text(gpt,text_to_token_ids(instruction,tokenizer),\n",
        "                                              max_tokens = 256,\n",
        "                                              content_size=1024)\n",
        "\n",
        "    response_text = token_ids_to_text(response_token_ids,tokenizer)\n",
        "    responce_text = responce_text[len(instruction):].replace(\"### Response\",\"\").strip()\n",
        "\n",
        "    test_data[i]['model_response'] = response_text\n",
        "\n",
        "    with open('instruction-data-with-response.json',\"w\") as file:\n",
        "        json.dump(test_data, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeWrdksRoucf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJXXxlXnoucg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96c714c6fbea4defa7624d27f92a1b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff0eed8893db43caa1187899a3748cba",
              "IPY_MODEL_f749676abf2f4a64bf7a86fde29cd927",
              "IPY_MODEL_b8d9b0338a954990a404d633a1970b7d"
            ],
            "layout": "IPY_MODEL_15f380cce4504d2f9436d37c3ca10e6a"
          }
        },
        "ff0eed8893db43caa1187899a3748cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc94455d2ec44e691f4ea160af745bb",
            "placeholder": "​",
            "style": "IPY_MODEL_51c6278f294e4714af5b3ab70a29e924",
            "value": "config.json: 100%"
          }
        },
        "f749676abf2f4a64bf7a86fde29cd927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92aeeb9100314a33abd1566e225ee5d7",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c0aa9e6437442cdb580f76b06c20f53",
            "value": 718
          }
        },
        "b8d9b0338a954990a404d633a1970b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27100473cb524661beb0b3028fdc72f9",
            "placeholder": "​",
            "style": "IPY_MODEL_2d822c0b58f5460bb8a10879d54ace6f",
            "value": " 718/718 [00:00&lt;00:00, 35.7kB/s]"
          }
        },
        "15f380cce4504d2f9436d37c3ca10e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc94455d2ec44e691f4ea160af745bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c6278f294e4714af5b3ab70a29e924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92aeeb9100314a33abd1566e225ee5d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c0aa9e6437442cdb580f76b06c20f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27100473cb524661beb0b3028fdc72f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d822c0b58f5460bb8a10879d54ace6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b980325f813454d9c0612de0e1c58e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3cf5921602e41f399895c404d3c562d",
              "IPY_MODEL_606da1500bda488fb01e37d20fb8616a",
              "IPY_MODEL_d0f5319126284db39e0a7d3089b10b51"
            ],
            "layout": "IPY_MODEL_72c265b1be13411396ca8921a23bd53f"
          }
        },
        "a3cf5921602e41f399895c404d3c562d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58c5c9e55a2c4773a213aa2f2a22af36",
            "placeholder": "​",
            "style": "IPY_MODEL_e6cd6068f8be42c2b1e9cd4f680edeb4",
            "value": "model.safetensors: 100%"
          }
        },
        "606da1500bda488fb01e37d20fb8616a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f35efe4d1412486fa9cef2f8321e8789",
            "max": 1519984962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19e78a4620024d9ca8e0ca5287b9f35a",
            "value": 1519984962
          }
        },
        "d0f5319126284db39e0a7d3089b10b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dfa9222ef394b129c0364437b1e7fc1",
            "placeholder": "​",
            "style": "IPY_MODEL_42cdc8cb3eab4a5ebf181ce9fac7cace",
            "value": " 1.52G/1.52G [00:06&lt;00:00, 242MB/s]"
          }
        },
        "72c265b1be13411396ca8921a23bd53f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c5c9e55a2c4773a213aa2f2a22af36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6cd6068f8be42c2b1e9cd4f680edeb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35efe4d1412486fa9cef2f8321e8789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e78a4620024d9ca8e0ca5287b9f35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dfa9222ef394b129c0364437b1e7fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42cdc8cb3eab4a5ebf181ce9fac7cace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}